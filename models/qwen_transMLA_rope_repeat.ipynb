{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/transmla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qwen2.modeling_qwen2 import Qwen2MLAForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.67s/it]\n",
      "Some weights of Qwen2MLAForCausalLM were not initialized from the model checkpoint at /root/mfx/huggingface/Qwen/Qwen2.5-3B and are newly initialized: ['model.layers.0.self_attn.k_up_proj.weight', 'model.layers.0.self_attn.v_up_proj.weight', 'model.layers.1.self_attn.k_up_proj.weight', 'model.layers.1.self_attn.v_up_proj.weight', 'model.layers.10.self_attn.k_up_proj.weight', 'model.layers.10.self_attn.v_up_proj.weight', 'model.layers.11.self_attn.k_up_proj.weight', 'model.layers.11.self_attn.v_up_proj.weight', 'model.layers.12.self_attn.k_up_proj.weight', 'model.layers.12.self_attn.v_up_proj.weight', 'model.layers.13.self_attn.k_up_proj.weight', 'model.layers.13.self_attn.v_up_proj.weight', 'model.layers.14.self_attn.k_up_proj.weight', 'model.layers.14.self_attn.v_up_proj.weight', 'model.layers.15.self_attn.k_up_proj.weight', 'model.layers.15.self_attn.v_up_proj.weight', 'model.layers.16.self_attn.k_up_proj.weight', 'model.layers.16.self_attn.v_up_proj.weight', 'model.layers.17.self_attn.k_up_proj.weight', 'model.layers.17.self_attn.v_up_proj.weight', 'model.layers.18.self_attn.k_up_proj.weight', 'model.layers.18.self_attn.v_up_proj.weight', 'model.layers.19.self_attn.k_up_proj.weight', 'model.layers.19.self_attn.v_up_proj.weight', 'model.layers.2.self_attn.k_up_proj.weight', 'model.layers.2.self_attn.v_up_proj.weight', 'model.layers.20.self_attn.k_up_proj.weight', 'model.layers.20.self_attn.v_up_proj.weight', 'model.layers.21.self_attn.k_up_proj.weight', 'model.layers.21.self_attn.v_up_proj.weight', 'model.layers.22.self_attn.k_up_proj.weight', 'model.layers.22.self_attn.v_up_proj.weight', 'model.layers.23.self_attn.k_up_proj.weight', 'model.layers.23.self_attn.v_up_proj.weight', 'model.layers.24.self_attn.k_up_proj.weight', 'model.layers.24.self_attn.v_up_proj.weight', 'model.layers.25.self_attn.k_up_proj.weight', 'model.layers.25.self_attn.v_up_proj.weight', 'model.layers.26.self_attn.k_up_proj.weight', 'model.layers.26.self_attn.v_up_proj.weight', 'model.layers.27.self_attn.k_up_proj.weight', 'model.layers.27.self_attn.v_up_proj.weight', 'model.layers.28.self_attn.k_up_proj.weight', 'model.layers.28.self_attn.v_up_proj.weight', 'model.layers.29.self_attn.k_up_proj.weight', 'model.layers.29.self_attn.v_up_proj.weight', 'model.layers.3.self_attn.k_up_proj.weight', 'model.layers.3.self_attn.v_up_proj.weight', 'model.layers.30.self_attn.k_up_proj.weight', 'model.layers.30.self_attn.v_up_proj.weight', 'model.layers.31.self_attn.k_up_proj.weight', 'model.layers.31.self_attn.v_up_proj.weight', 'model.layers.32.self_attn.k_up_proj.weight', 'model.layers.32.self_attn.v_up_proj.weight', 'model.layers.33.self_attn.k_up_proj.weight', 'model.layers.33.self_attn.v_up_proj.weight', 'model.layers.34.self_attn.k_up_proj.weight', 'model.layers.34.self_attn.v_up_proj.weight', 'model.layers.35.self_attn.k_up_proj.weight', 'model.layers.35.self_attn.v_up_proj.weight', 'model.layers.4.self_attn.k_up_proj.weight', 'model.layers.4.self_attn.v_up_proj.weight', 'model.layers.5.self_attn.k_up_proj.weight', 'model.layers.5.self_attn.v_up_proj.weight', 'model.layers.6.self_attn.k_up_proj.weight', 'model.layers.6.self_attn.v_up_proj.weight', 'model.layers.7.self_attn.k_up_proj.weight', 'model.layers.7.self_attn.v_up_proj.weight', 'model.layers.8.self_attn.k_up_proj.weight', 'model.layers.8.self_attn.v_up_proj.weight', 'model.layers.9.self_attn.k_up_proj.weight', 'model.layers.9.self_attn.v_up_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2MLAForCausalLM(\n",
       "  (model): Qwen2MLAModel(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2MLAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_up_proj): Linear(in_features=256, out_features=2048, bias=False)\n",
       "          (v_up_proj): Linear(in_features=256, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Qwen2MLAForCausalLM.from_pretrained(\"/root/mfx/huggingface/Qwen/Qwen2.5-3B\", device_map='cuda:4', attn_implementation=\"eager\", partial_rotary_factor=1, rope_repeat=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/mfx/huggingface/Qwen/Qwen2.5-3B\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "n_heads = model.config.num_attention_heads\n",
    "kv_heads = model.config.num_key_value_heads\n",
    "head_dim = model.config.hidden_size//model.config.num_attention_heads\n",
    "latent_dim = kv_heads * head_dim\n",
    "kv_groups = model.config.num_attention_heads // model.config.num_key_value_heads\n",
    "model.config.partial_rotary_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert identity matrices\n",
    "for name,module in model.named_modules():\n",
    "    if 'k_up_proj' in name or \"v_up_proj\" in name:\n",
    "        module.weight.data = torch.stack([torch.eye(latent_dim).reshape(kv_heads, head_dim, latent_dim)]*kv_groups,dim=1).reshape(hidden_size, latent_dim).contiguous().to(module.weight.data.device,module.weight.data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个故事吧。\n",
      "当然可以，这是一个关于勇气和友谊的故事。\n",
      "\n",
      "从前，在一个遥远的森林里，住着一只名叫蓝羽的小鸟。蓝羽拥有一身美丽的蓝色羽毛，但它总是感到孤独，因为它没有朋友。每天，它都在森林中飞来飞去，寻找可以交朋友的伙伴。\n",
      "\n",
      "一天，蓝羽遇到了一只受伤的小兔子，名叫米洛。米洛的腿受伤了，无法行走。蓝羽立刻停下来，用它的翅膀轻轻地拍打着地面，帮助米洛站起来。米洛感激地看着蓝羽，说：“谢谢你，蓝羽。你真是个好伙伴。”\n",
      "\n",
      "从那天起，蓝羽和米洛成了最好的朋友。他们一起探索森林，分享彼此的故事和秘密。蓝羽教会了米洛如何飞翔，而米洛则教给了蓝羽如何照顾受伤的动物。\n",
      "\n",
      "随着时间的推移，森林里的其他动物也开始注意到蓝羽和米洛之间的特殊关系。它们邀请蓝羽和米洛参加各种活动，包括音乐会、舞蹈比赛和野餐。蓝羽和米洛都非常开心，因为他们知道，有了朋友，生活变得更加美好。\n",
      "\n",
      "然而，好景不长，森林里来了一只凶猛的狼，它威胁着所有动物的安全。蓝羽和米洛决定联手对抗这只狼。他们利用自己的智慧和勇气，成功地将狼赶走了。\n",
      "\n",
      "从那以后，蓝羽和米洛成为了森林中的英雄。他们的友谊证明了，即使是最小的生物也能拥有巨大的力量和勇气。而蓝羽也明白了，真正的友谊是无价的，它能够带来快乐、支持和力量。\n",
      "\n",
      "这个故事告诉我们，无论我们多么渺小，只要我们有勇气去帮助别人，就有机会找到真正的友谊和幸福。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"给我讲一个故事吧\",return_tensors=\"pt\").to(\"cuda:4\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,module in model.named_modules():\n",
    "    if name.endswith(\"self_attn\"):\n",
    "        # Orthogonal q_proj and k_up_proj\n",
    "        k_up_weight = deepcopy(module.k_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        q_weight = deepcopy(module.q_proj.weight.data).reshape(n_heads, head_dim, hidden_size) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_weight = torch.cat([q_weight,deepcopy(module.q_proj.bias.data).reshape(n_heads, head_dim, 1)],dim=-1)\n",
    "        q_k_up = torch.einsum(\"hdc,hdD->hcD\",k_up_weight, q_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        U,S,V = torch.svd_lowrank(q_k_up, head_dim, niter=16) # U(n_heads, latent_dim, head_dim), S(n_heads, head_dim), V(n_heads, hidden_size, head_dim)\n",
    "        US_sqrt = torch.einsum('hLd,hd->hdL',U,torch.sqrt(S)) # (n_heads, head_dim, latent_dim)\n",
    "        S_sqrtV = torch.einsum('hd,hDd->hdD',torch.sqrt(S),V) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            module.q_proj.bias.data = S_sqrtV[:,:,-1].reshape(-1).contiguous()\n",
    "            S_sqrtV = S_sqrtV[:,:,:-1]\n",
    "        module.k_up_proj.weight.data = US_sqrt.reshape(n_heads*head_dim, latent_dim).contiguous()\n",
    "        module.q_proj.weight.data = S_sqrtV.reshape(n_heads*head_dim, hidden_size).contiguous()\n",
    "\n",
    "        # Orthogonal o_proj and v_up_proj\n",
    "        v_up_weight = deepcopy(module.v_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim)\n",
    "        o_weight = deepcopy(module.o_proj.weight.data).reshape(hidden_size, n_heads, head_dim)\n",
    "        v_up_o = torch.einsum(\"hdc,Dhd->hcD\",v_up_weight, o_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        U,S,V = torch.svd_lowrank(v_up_o, head_dim, niter=16) # U(n_heads, latent_dim, head_dim), S(n_heads, head_dim), V(n_heads, hidden_size, head_dim)\n",
    "        US_sqrt = torch.einsum('hLd,hd->hdL',U,torch.sqrt(S)) # (n_heads, head_dim, latent_dim)\n",
    "        S_sqrtV = torch.einsum('hd,hDd->Dhd',torch.sqrt(S),V) # (hidden_size, n_heads, head_dim)\n",
    "        module.v_up_proj.weight.data = US_sqrt.reshape(hidden_size, latent_dim).contiguous()\n",
    "        module.o_proj.weight.data = S_sqrtV.reshape(hidden_size, n_heads*head_dim).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个故事吧。\n",
      "当然可以，这是一个关于勇气和友谊的故事。\n",
      "\n",
      "从前，在一个遥远的森林里，住着一只名叫蓝羽的小鸟。蓝羽拥有一身美丽的蓝色羽毛，但它总是感到孤独，因为它没有朋友。每天，它都在森林中飞来飞去，寻找可以交朋友的伙伴。\n",
      "\n",
      "一天，蓝羽遇到了一只受伤的小兔子，名叫米洛。米洛的腿受伤了，无法行走。蓝羽立刻停下来，用它的翅膀轻轻地拍打着地面，帮助米洛站起来。米洛感激地看着蓝羽，说：“谢谢你，蓝羽。你真是个好伙伴。”\n",
      "\n",
      "从那天起，蓝羽和米洛成了最好的朋友。他们一起探索森林，分享彼此的故事和秘密。蓝羽教会了米洛如何飞翔，而米洛则教给了蓝羽如何照顾受伤的动物。\n",
      "\n",
      "随着时间的推移，森林里的其他动物也开始注意到蓝羽和米洛之间的特殊关系。它们邀请蓝羽和米洛参加各种活动，包括音乐会、舞蹈比赛和野餐。蓝羽和米洛都非常开心，因为他们知道，有了朋友，生活变得更加美好。\n",
      "\n",
      "然而，好景不长，森林里来了一只凶猛的狼，它威胁着所有动物的安全。蓝羽和米洛决定联手对抗这只狼。他们利用自己的智慧和勇气，成功地将狼赶走了。\n",
      "\n",
      "从那以后，蓝羽和米洛成为了森林中的英雄。他们的友谊证明了，即使是最小的生物也能拥有巨大的力量和勇气。而蓝羽也明白了，真正的友谊是无价的，它能够带来快乐、支持和力量。\n",
      "\n",
      "这个故事告诉我们，无论我们多么渺小，只要我们有勇气去帮助别人，就有机会找到真正的友谊和幸福。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"给我讲一个故事吧\",return_tensors=\"pt\").to(\"cuda:4\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,module in model.named_modules():\n",
    "    if name.endswith(\"self_attn\"):\n",
    "        # Absorb k_up_proj into q_proj\n",
    "        k_up_weight = deepcopy(module.k_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        q_weight = deepcopy(module.q_proj.weight.data).reshape(n_heads, head_dim, hidden_size) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_weight = torch.cat([q_weight,deepcopy(module.q_proj.bias.data).reshape(n_heads, head_dim, 1)],dim=-1)\n",
    "        q_k_up = torch.einsum(\"hdc,hdD->hcD\",k_up_weight, q_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        q_proj = torch.nn.Linear(hidden_size, n_heads*latent_dim, bias=(module.q_proj.bias is not None))\n",
    "        q_proj = q_proj.to(device=module.q_proj.weight.device, dtype=module.q_proj.weight.dtype)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_proj.bias.data = q_k_up[:,:,-1].reshape(-1).contiguous()\n",
    "            q_k_up = q_k_up[:,:,:-1]\n",
    "        q_proj.weight.data = q_k_up.reshape(n_heads*latent_dim, hidden_size).contiguous()\n",
    "        setattr(module, \"q_proj\", q_proj)\n",
    "        delattr(module, \"k_up_proj\")\n",
    "        # Absorb v_up_proj into o_proj\n",
    "        v_up_weight = deepcopy(module.v_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        o_weight = deepcopy(module.o_proj.weight.data).reshape(hidden_size, n_heads, head_dim) # (n_heads, head_dim, hidden_size)\n",
    "        v_up_o = torch.einsum(\"hdc,Dhd->Dhc\",v_up_weight, o_weight) # (hidden_size, n_heads, latent_dim), rank<=head_dim\n",
    "        o_proj = torch.nn.Linear(n_heads*latent_dim, hidden_size, bias=(module.o_proj.bias is not None))\n",
    "        o_proj = o_proj.to(device=module.o_proj.weight.device, dtype=module.o_proj.weight.dtype)\n",
    "        o_proj.weight.data = v_up_o.reshape(hidden_size, n_heads*latent_dim).contiguous()\n",
    "        if module.o_proj.bias is not None:\n",
    "            o_proj.bias.data = module.o_proj.bias\n",
    "        setattr(module, \"o_proj\", o_proj)\n",
    "        delattr(module, \"v_up_proj\")\n",
    "        module.absorb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个故事吧。\n",
      "当然可以，这是一个关于勇气和友谊的故事。\n",
      "\n",
      "从前，在一个遥远的森林里，住着一只名叫蓝羽的小鸟。蓝羽拥有一身美丽的蓝色羽毛，但它总是感到孤独，因为它没有朋友。每天，它都在森林中飞来飞去，寻找可以交朋友的伙伴。\n",
      "\n",
      "一天，蓝羽遇到了一只受伤的小兔子，名叫米洛。米洛的腿受伤了，无法行走。蓝羽立刻停下来，用它的翅膀轻轻地拍打着地面，帮助米洛站起来。米洛感激地看着蓝羽，说：“谢谢你，蓝羽。你真是个好伙伴。”\n",
      "\n",
      "从那天起，蓝羽和米洛成了最好的朋友。他们一起探索森林，分享彼此的故事和秘密。蓝羽教会了米洛如何飞翔，而米洛则教给了蓝羽如何照顾受伤的动物。\n",
      "\n",
      "随着时间的推移，森林里的其他动物也开始注意到蓝羽和米洛之间的特殊关系。它们邀请蓝羽和米洛参加各种活动，包括音乐会、舞蹈比赛和野餐。蓝羽和米洛都非常开心，因为他们知道，有了朋友，生活变得更加美好。\n",
      "\n",
      "然而，好景不长，森林里来了一只凶猛的狼，它威胁着所有动物的安全。蓝羽和米洛决定联手对抗这只狼。他们利用自己的智慧和勇气，成功地将狼赶走了。\n",
      "\n",
      "从那以后，蓝羽和米洛成为了森林中的英雄。他们的友谊证明了，即使是最小的生物也能拥有巨大的力量和勇气。而蓝羽也明白了，真正的友谊是无价的，它能够带来快乐、支持和力量。\n",
      "\n",
      "这个故事告诉我们，无论我们多么渺小，只要我们有勇气去帮助别人，就有机会找到真正的友谊和幸福。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"给我讲一个故事吧\",return_tensors=\"pt\").to(\"cuda:4\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transmla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
