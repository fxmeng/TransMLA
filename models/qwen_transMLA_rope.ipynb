{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/transmla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qwen2.modeling_qwen2 import Qwen2MLAForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n",
      "Some weights of Qwen2MLAForCausalLM were not initialized from the model checkpoint at /root/mfx/huggingface/Qwen/Qwen2.5-3B and are newly initialized: ['model.layers.0.self_attn.k_up_proj.weight', 'model.layers.0.self_attn.v_up_proj.weight', 'model.layers.1.self_attn.k_up_proj.weight', 'model.layers.1.self_attn.v_up_proj.weight', 'model.layers.10.self_attn.k_up_proj.weight', 'model.layers.10.self_attn.v_up_proj.weight', 'model.layers.11.self_attn.k_up_proj.weight', 'model.layers.11.self_attn.v_up_proj.weight', 'model.layers.12.self_attn.k_up_proj.weight', 'model.layers.12.self_attn.v_up_proj.weight', 'model.layers.13.self_attn.k_up_proj.weight', 'model.layers.13.self_attn.v_up_proj.weight', 'model.layers.14.self_attn.k_up_proj.weight', 'model.layers.14.self_attn.v_up_proj.weight', 'model.layers.15.self_attn.k_up_proj.weight', 'model.layers.15.self_attn.v_up_proj.weight', 'model.layers.16.self_attn.k_up_proj.weight', 'model.layers.16.self_attn.v_up_proj.weight', 'model.layers.17.self_attn.k_up_proj.weight', 'model.layers.17.self_attn.v_up_proj.weight', 'model.layers.18.self_attn.k_up_proj.weight', 'model.layers.18.self_attn.v_up_proj.weight', 'model.layers.19.self_attn.k_up_proj.weight', 'model.layers.19.self_attn.v_up_proj.weight', 'model.layers.2.self_attn.k_up_proj.weight', 'model.layers.2.self_attn.v_up_proj.weight', 'model.layers.20.self_attn.k_up_proj.weight', 'model.layers.20.self_attn.v_up_proj.weight', 'model.layers.21.self_attn.k_up_proj.weight', 'model.layers.21.self_attn.v_up_proj.weight', 'model.layers.22.self_attn.k_up_proj.weight', 'model.layers.22.self_attn.v_up_proj.weight', 'model.layers.23.self_attn.k_up_proj.weight', 'model.layers.23.self_attn.v_up_proj.weight', 'model.layers.24.self_attn.k_up_proj.weight', 'model.layers.24.self_attn.v_up_proj.weight', 'model.layers.25.self_attn.k_up_proj.weight', 'model.layers.25.self_attn.v_up_proj.weight', 'model.layers.26.self_attn.k_up_proj.weight', 'model.layers.26.self_attn.v_up_proj.weight', 'model.layers.27.self_attn.k_up_proj.weight', 'model.layers.27.self_attn.v_up_proj.weight', 'model.layers.28.self_attn.k_up_proj.weight', 'model.layers.28.self_attn.v_up_proj.weight', 'model.layers.29.self_attn.k_up_proj.weight', 'model.layers.29.self_attn.v_up_proj.weight', 'model.layers.3.self_attn.k_up_proj.weight', 'model.layers.3.self_attn.v_up_proj.weight', 'model.layers.30.self_attn.k_up_proj.weight', 'model.layers.30.self_attn.v_up_proj.weight', 'model.layers.31.self_attn.k_up_proj.weight', 'model.layers.31.self_attn.v_up_proj.weight', 'model.layers.32.self_attn.k_up_proj.weight', 'model.layers.32.self_attn.v_up_proj.weight', 'model.layers.33.self_attn.k_up_proj.weight', 'model.layers.33.self_attn.v_up_proj.weight', 'model.layers.34.self_attn.k_up_proj.weight', 'model.layers.34.self_attn.v_up_proj.weight', 'model.layers.35.self_attn.k_up_proj.weight', 'model.layers.35.self_attn.v_up_proj.weight', 'model.layers.4.self_attn.k_up_proj.weight', 'model.layers.4.self_attn.v_up_proj.weight', 'model.layers.5.self_attn.k_up_proj.weight', 'model.layers.5.self_attn.v_up_proj.weight', 'model.layers.6.self_attn.k_up_proj.weight', 'model.layers.6.self_attn.v_up_proj.weight', 'model.layers.7.self_attn.k_up_proj.weight', 'model.layers.7.self_attn.v_up_proj.weight', 'model.layers.8.self_attn.k_up_proj.weight', 'model.layers.8.self_attn.v_up_proj.weight', 'model.layers.9.self_attn.k_up_proj.weight', 'model.layers.9.self_attn.v_up_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2MLAForCausalLM(\n",
       "  (model): Qwen2MLAModel(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaMLAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_up_proj): Linear(in_features=256, out_features=2048, bias=False)\n",
       "          (v_up_proj): Linear(in_features=256, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Qwen2MLAForCausalLM.from_pretrained(\"/root/mfx/huggingface/Qwen/Qwen2.5-3B\", device_map='cuda:7', attn_implementation=\"sdpa\", partial_rotary_factor=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/mfx/huggingface/Qwen/Qwen2.5-3B\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "n_heads = model.config.num_attention_heads\n",
    "kv_heads = model.config.num_key_value_heads\n",
    "head_dim = model.config.hidden_size//model.config.num_attention_heads\n",
    "latent_dim = kv_heads * head_dim\n",
    "kv_groups = model.config.num_attention_heads // model.config.num_key_value_heads\n",
    "model.config.partial_rotary_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert identity matrices\n",
    "for name,module in model.named_modules():\n",
    "    if 'k_up_proj' in name or \"v_up_proj\" in name:\n",
    "        weight = torch.stack([torch.eye(latent_dim).reshape(kv_heads, head_dim, latent_dim)]*kv_groups,dim=1).reshape(hidden_size, latent_dim).contiguous().to(module.weight.data.device,module.weight.data.dtype)\n",
    "        if 'k_up_proj' in name:\n",
    "            weight = weight.view(hidden_size, kv_heads, head_dim).transpose(1,2).contiguous().view(hidden_size, latent_dim)\n",
    "        module.weight.data=weight\n",
    "    elif 'k_proj' in name:\n",
    "        module.weight.data = module.weight.data.view(kv_heads, head_dim, hidden_size).transpose(0,1).contiguous().view(latent_dim, hidden_size)\n",
    "        module.bias.data = module.bias.data.view(kv_heads, head_dim).transpose(0,1).contiguous().view(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个故事吧。\n",
      "当然可以，这是一个关于勇气和友谊的故事。\n",
      "\n",
      "从前，在一个遥远的森林里，住着一只名叫蓝羽的小鸟。蓝羽拥有一身美丽的蓝色羽毛，但它总是感到孤独，因为它没有朋友。一天，森林里来了一只受伤的小兔子，名叫米洛。米洛的腿受伤了，无法行走。蓝羽看到米洛的痛苦，决定帮助它。\n",
      "\n",
      "蓝羽用它的翅膀轻轻地拍打着地面，帮助米洛站起来。米洛感激地看着蓝羽，说：“谢谢你，蓝羽。你真是个勇敢的朋友。”从那天起，蓝羽和米洛成了最好的朋友。他们一起探索森林，分享彼此的故事和秘密。\n",
      "\n",
      "有一天，森林里发生了一场大火，许多动物都逃到了安全的地方。蓝羽和米洛也加入了逃跑的队伍。在逃跑的过程中，他们遇到了一只凶猛的野狼。野狼想要吃掉他们，但蓝羽和米洛并没有害怕。他们一起合作，用智慧和勇气打败了野狼。\n",
      "\n",
      "从那以后，蓝羽和米洛变得更加勇敢和坚强。他们知道，只要有朋友在身边，就没有克服不了的困难。而他们的友谊也变得更加深厚，成为了森林中最美好的传说。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"给我讲一个故事吧\",return_tensors=\"pt\").to(\"cuda:7\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,module in model.named_modules():\n",
    "    if name.endswith(\"self_attn\"):\n",
    "        # Orthogonal q_proj and k_up_proj\n",
    "        k_up_weight = deepcopy(module.k_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        q_weight = deepcopy(module.q_proj.weight.data).reshape(n_heads, head_dim, hidden_size) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_weight = torch.cat([q_weight,deepcopy(module.q_proj.bias.data).reshape(n_heads, head_dim, 1)],dim=-1)\n",
    "        q_k_up = torch.einsum(\"hdc,hdD->hcD\",k_up_weight, q_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        U,S,V = torch.svd_lowrank(q_k_up, head_dim, niter=16) # U(n_heads, latent_dim, head_dim), S(n_heads, head_dim), V(n_heads, hidden_size, head_dim)\n",
    "        US_sqrt = torch.einsum('hLd,hd->hdL',U,torch.sqrt(S)) # (n_heads, head_dim, latent_dim)\n",
    "        S_sqrtV = torch.einsum('hd,hDd->hdD',torch.sqrt(S),V) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            module.q_proj.bias.data = S_sqrtV[:,:,-1].reshape(-1).contiguous()\n",
    "            S_sqrtV = S_sqrtV[:,:,:-1]\n",
    "        module.k_up_proj.weight.data = US_sqrt.reshape(n_heads*head_dim, latent_dim).contiguous()\n",
    "        module.q_proj.weight.data = S_sqrtV.reshape(n_heads*head_dim, hidden_size).contiguous()\n",
    "\n",
    "        # Orthogonal o_proj and v_up_proj\n",
    "        v_up_weight = deepcopy(module.v_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim)\n",
    "        o_weight = deepcopy(module.o_proj.weight.data).reshape(hidden_size, n_heads, head_dim)\n",
    "        v_up_o = torch.einsum(\"hdc,Dhd->hcD\",v_up_weight, o_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        U,S,V = torch.svd_lowrank(v_up_o, head_dim, niter=16) # U(n_heads, latent_dim, head_dim), S(n_heads, head_dim), V(n_heads, hidden_size, head_dim)\n",
    "        US_sqrt = torch.einsum('hLd,hd->hdL',U,torch.sqrt(S)) # (n_heads, head_dim, latent_dim)\n",
    "        S_sqrtV = torch.einsum('hd,hDd->Dhd',torch.sqrt(S),V) # (hidden_size, n_heads, head_dim)\n",
    "        module.v_up_proj.weight.data = US_sqrt.reshape(hidden_size, latent_dim).contiguous()\n",
    "        module.o_proj.weight.data = S_sqrtV.reshape(hidden_size, n_heads*head_dim).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个故事吧。\n",
      "当然可以，这是一个关于勇气和友谊的故事。\n",
      "\n",
      "从前，在一个遥远的森林里，住着一只名叫蓝羽的小鸟。蓝羽拥有一身美丽的蓝色羽毛，但它总是感到孤独，因为它没有朋友。一天，森林里来了一只受伤的小兔子，名叫米洛。米洛的腿受伤了，无法行走。蓝羽看到米洛的痛苦，决定帮助它。\n",
      "\n",
      "蓝羽用它的翅膀轻轻地拍打着地面，帮助米洛站起来。米洛感激地看着蓝羽，说：“谢谢你，蓝羽。你真是个勇敢的朋友。”从那天起，蓝羽和米洛成了最好的朋友。他们一起探索森林，分享彼此的故事和秘密。\n",
      "\n",
      "有一天，森林里发生了一场大火，许多动物都逃到了安全的地方。蓝羽和米洛也加入了逃跑的队伍。在逃跑的过程中，他们遇到了一只凶猛的野狼。野狼想要吃掉他们，但蓝羽和米洛并没有害怕。他们一起合作，用智慧和勇气打败了野狼。\n",
      "\n",
      "从那以后，蓝羽和米洛变得更加勇敢和坚强。他们知道，只要有朋友在身边，就没有克服不了的困难。而他们的友谊也变得更加深厚，成为了森林中最美好的传说。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"给我讲一个故事吧\",return_tensors=\"pt\").to(\"cuda:7\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,module in model.named_modules():\n",
    "    if name.endswith(\"self_attn\"):\n",
    "        # Absorb k_up_proj into q_proj\n",
    "        k_up_weight = deepcopy(module.k_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        q_weight = deepcopy(module.q_proj.weight.data).reshape(n_heads, head_dim, hidden_size) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_weight = torch.cat([q_weight,deepcopy(module.q_proj.bias.data).reshape(n_heads, head_dim, 1)],dim=-1)\n",
    "        q_k_up = torch.einsum(\"hdc,hdD->hcD\",k_up_weight, q_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        q_proj = torch.nn.Linear(hidden_size, n_heads*latent_dim, bias=(module.q_proj.bias is not None))\n",
    "        q_proj = q_proj.to(device=module.q_proj.weight.device, dtype=module.q_proj.weight.dtype)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_proj.bias.data = q_k_up[:,:,-1].reshape(-1).contiguous()\n",
    "            q_k_up = q_k_up[:,:,:-1]\n",
    "        q_proj.weight.data = q_k_up.reshape(n_heads*latent_dim, hidden_size).contiguous()\n",
    "        setattr(module, \"q_proj\", q_proj)\n",
    "        delattr(module, \"k_up_proj\")\n",
    "        # Absorb v_up_proj into o_proj\n",
    "        v_up_weight = deepcopy(module.v_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        o_weight = deepcopy(module.o_proj.weight.data).reshape(hidden_size, n_heads, head_dim) # (n_heads, head_dim, hidden_size)\n",
    "        v_up_o = torch.einsum(\"hdc,Dhd->Dhc\",v_up_weight, o_weight) # (hidden_size, n_heads, latent_dim), rank<=head_dim\n",
    "        o_proj = torch.nn.Linear(n_heads*latent_dim, hidden_size, bias=(module.o_proj.bias is not None))\n",
    "        o_proj = o_proj.to(device=module.o_proj.weight.device, dtype=module.o_proj.weight.dtype)\n",
    "        o_proj.weight.data = v_up_o.reshape(hidden_size, n_heads*latent_dim).contiguous()\n",
    "        if module.o_proj.bias is not None:\n",
    "            o_proj.bias.data = module.o_proj.bias\n",
    "        setattr(module, \"o_proj\", o_proj)\n",
    "        delattr(module, \"v_up_proj\")\n",
    "        module.absorb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个故事吧。\n",
      "当然可以，这是一个关于勇气和友谊的故事。\n",
      "\n",
      "从前，在一个遥远的森林里，住着一只名叫蓝羽的小鸟。蓝羽拥有一身美丽的蓝色羽毛，但它总是感到孤独，因为它没有朋友。一天，森林里来了一只受伤的小兔子，名叫米洛。米洛的腿受伤了，无法行走。蓝羽看到米洛的痛苦，决定帮助它。\n",
      "\n",
      "蓝羽用它的翅膀轻轻地拍打着地面，帮助米洛站起来。米洛感激地看着蓝羽，说：“谢谢你，蓝羽。你真是个勇敢的朋友。”从那天起，蓝羽和米洛成了最好的朋友。他们一起探索森林，分享彼此的故事和秘密。\n",
      "\n",
      "有一天，森林里发生了一场大火，许多动物都逃到了安全的地方。蓝羽和米洛也加入了逃跑的队伍。在逃跑的过程中，他们遇到了一只凶猛的野狼。野狼想要吃掉他们，但蓝羽和米洛并没有害怕。他们一起合作，用智慧和勇气打败了野狼。\n",
      "\n",
      "从那以后，蓝羽和米洛变得更加勇敢和坚强。他们知道，只要有朋友在身边，就没有克服不了的困难。而他们的友谊也变得更加深厚，成为了森林中最美好的传说。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"给我讲一个故事吧\",return_tensors=\"pt\").to(\"cuda:7\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.absorb=True\n",
    "model.config.head_dim=256\n",
    "model.config.latent_dim_factor=2\n",
    "model.config.num_key_value_heads=1\n",
    "model.config.partial_rotary_factor=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 00:52:59,757] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mfx/miniconda3/envs/transmla/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data/mfx/miniconda3/envs/transmla/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/transmla/lib/libcufile.so: undefined reference to `dlvsym'\n",
      "/data/mfx/miniconda3/envs/transmla/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/transmla/lib/libcufile.so: undefined reference to `dlopen'\n",
      "/data/mfx/miniconda3/envs/transmla/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/transmla/lib/libcufile.so: undefined reference to `dlclose'\n",
      "/data/mfx/miniconda3/envs/transmla/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/transmla/lib/libcufile.so: undefined reference to `dlerror'\n",
      "/data/mfx/miniconda3/envs/transmla/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /root/miniconda3/envs/transmla/lib/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('TransMLA_Qwen2.5-3B_absorb/tokenizer_config.json',\n",
       " 'TransMLA_Qwen2.5-3B_absorb/special_tokens_map.json',\n",
       " 'TransMLA_Qwen2.5-3B_absorb/vocab.json',\n",
       " 'TransMLA_Qwen2.5-3B_absorb/merges.txt',\n",
       " 'TransMLA_Qwen2.5-3B_absorb/added_tokens.json',\n",
       " 'TransMLA_Qwen2.5-3B_absorb/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.absorb=True\n",
    "model.save_pretrained(\"TransMLA_Qwen2.5-3B_absorb\")\n",
    "tokenizer.save_pretrained(\"TransMLA_Qwen2.5-3B_absorb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen2.modeling_qwen2 import Qwen2MLAForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2MLAForCausalLM(\n",
       "  (model): Qwen2MLAModel(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2MLAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Qwen2MLAForCausalLM.from_pretrained(\"TransMLA_Qwen2.5-3B_absorb\", device_map='cuda:7', attn_implementation=\"eager\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TransMLA_Qwen2.5-3B_absorb\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个故事吧。\n",
      "当然可以，这是一个关于勇气和友谊的故事。\n",
      "\n",
      "从前，在一个遥远的森林里，住着一只名叫蓝羽的小鸟。蓝羽拥有一身美丽的蓝色羽毛，但它总是感到孤独，因为它没有朋友。一天，森林里来了一只受伤的小兔子，名叫米洛。米洛的腿受伤了，无法行走。蓝羽看到米洛的痛苦，决定帮助它。\n",
      "\n",
      "蓝羽用它的翅膀轻轻地拍打着地面，帮助米洛站起来。米洛感激地看着蓝羽，说：“谢谢你，蓝羽。你真是个勇敢的朋友。”从那天起，蓝羽和米洛成了最好的朋友。他们一起探索森林，分享彼此的故事和秘密。\n",
      "\n",
      "有一天，森林里发生了一场大火，许多动物都逃到了安全的地方。蓝羽和米洛也加入了逃跑的队伍。在逃跑的过程中，他们遇到了一只凶猛的野狼。野狼想要吃掉他们，但蓝羽和米洛并没有害怕。他们一起合作，用智慧和勇气打败了野狼。\n",
      "\n",
      "从那以后，蓝羽和米洛变得更加勇敢和坚强。他们知道，只要有朋友在身边，就没有克服不了的困难。而他们的友谊也变得更加深厚，成为了森林中最美好的传说。<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"给我讲一个故事吧\",return_tensors=\"pt\").to(\"cuda:7\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:29,283\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import ModelRegistry\n",
    "from qwen2.vllm_qwen2 import Qwen2MLAForCausalLM\n",
    "ModelRegistry.register_model(\"Qwen2MLAForCausalLM\", Qwen2MLAForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 00:54:29 config.py:2272] Downcasting torch.float32 to torch.float16.\n",
      "INFO 02-19 00:54:29 config.py:510] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 02-19 00:54:29 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='TransMLA_Qwen2.5-3B_absorb', speculative_config=None, tokenizer='TransMLA_Qwen2.5-3B_absorb', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=TransMLA_Qwen2.5-3B_absorb, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-19 00:54:30 selector.py:120] Using Flash Attention backend.\n",
      "INFO 02-19 00:54:31 model_runner.py:1094] Starting to load model TransMLA_Qwen2.5-3B_absorb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.26it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.11it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 00:54:34 model_runner.py:1099] Loading model weights took 6.3619 GB\n",
      "INFO 02-19 00:54:35 worker.py:241] Memory profiling takes 1.48 seconds\n",
      "INFO 02-19 00:54:35 worker.py:241] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n",
      "INFO 02-19 00:54:35 worker.py:241] model weights take 6.36GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.52GiB; the rest of the memory reserved for KV Cache is 26.46GiB.\n",
      "INFO 02-19 00:54:35 gpu_executor.py:76] # GPU blocks: 48165, # CPU blocks: 7281\n",
      "INFO 02-19 00:54:35 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 23.52x\n",
      "INFO 02-19 00:54:38 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 00:54:52 model_runner.py:1535] Graph capturing finished in 15 secs, took 0.23 GiB\n",
      "INFO 02-19 00:54:52 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 18.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Currently only support tensor_parallel_size=1\n",
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\"TransMLA_Qwen2.5-3B_absorb\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it, est. speed input: 2.06 toks/s, output: 104.28 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个故事吧。\n",
      "当然可以，这是一个关于勇气和友谊的故事。\n",
      "\n",
      "从前，在一个遥远的森林里，住着一只名叫蓝羽的小鸟。蓝羽拥有一身美丽的蓝色羽毛，但它总是感到孤独，因为它没有朋友。一天，森林里来了一只受伤的小兔子，名叫米洛。米洛的腿受伤了，无法行走。蓝羽看到米洛的痛苦，决定帮助它。\n",
      "\n",
      "蓝羽用它的翅膀轻轻地拍打着地面，帮助米洛站起来。米洛感激地看着蓝羽，说：“谢谢你，蓝羽。你真是个勇敢的朋友。”从那天起，蓝羽和米洛成了最好的朋友。他们一起探索森林，分享彼此的故事和秘密。\n",
      "\n",
      "有一天，森林里发生了一场大火，许多动物都逃到了安全的地方。蓝羽和米洛也加入了逃跑的队伍。在逃跑的过程中，他们遇到了一只凶猛的野狼。野狼想要吃掉他们，但蓝羽和米洛并没有害怕。他们一起合作，用智慧和勇气打败了野狼。\n",
      "\n",
      "从那以后，蓝羽和米洛变得更加勇敢和坚强。他们知道，只要有朋友在身边，就没有克服不了的困难。而他们的友谊也变得更加深厚，成为了森林中最美好的传说。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"给我讲一个故事吧\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=512)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(prompt+generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transmla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
