{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/transmla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama.modeling_llama import LlamaForCausalLM\n",
    "from llama.configuration_llama import LlamaConfig\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.70s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /root/mfx/huggingface/meta-llama/Meta-Llama-3-8B and are newly initialized: ['model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_up_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_up_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_up_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_up_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_up_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_up_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_up_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_up_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_up_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.12.self_attn.v_up_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_up_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_up_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_up_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_up_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_up_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_up_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_up_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_up_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_up_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_up_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_up_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_up_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_up_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_up_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_up_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_up_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_up_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_up_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_up_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_up_proj.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_up_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_up_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_up_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_up_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_up_proj.weight', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_up_proj.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_up_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_up_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_up_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_up_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_up_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_up_proj.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_up_proj.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.28.self_attn.v_up_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_up_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_up_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_up_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_up_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_up_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_up_proj.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_up_proj.weight', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_up_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_up_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_up_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_up_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_up_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_up_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_up_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_up_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_up_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_up_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.8.self_attn.v_up_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_up_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_up_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaMLAttention(\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (v_up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\"/root/mfx/huggingface/meta-llama/Meta-Llama-3-8B\", device_map='auto', attn_implementation=\"sdpa\", partial_rotary_factor=1, rope_repeat=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/mfx/huggingface/meta-llama/Meta-Llama-3-8B\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "n_heads = model.config.num_attention_heads\n",
    "kv_heads = model.config.num_key_value_heads\n",
    "head_dim = model.config.hidden_size//model.config.num_attention_heads\n",
    "latent_dim = kv_heads * head_dim\n",
    "kv_groups = model.config.num_attention_heads // model.config.num_key_value_heads\n",
    "model.config.partial_rotary_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert identity matrices\n",
    "for name,module in model.named_modules():\n",
    "    if 'k_up_proj' in name or \"v_up_proj\" in name:\n",
    "        module.weight.data = torch.stack([torch.eye(latent_dim).reshape(kv_heads, head_dim, latent_dim)]*kv_groups,dim=1).reshape(hidden_size, latent_dim).contiguous().to(module.weight.data.device,module.weight.data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/transmla/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/transmla/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Tall me a story\n",
      "I am a tall woman. I am 5'10\" and I have been tall my whole life. I have always been the tallest person in the room. I have always been the tallest person in my family. I have always been the tallest person in my class. I have always been the tallest person in my group of friends. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"Tall me a story\",return_tensors=\"pt\").to(\"cuda:1\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,module in model.named_modules():\n",
    "    if name.endswith(\"self_attn\"):\n",
    "        # Orthogonal q_proj and k_up_proj\n",
    "        k_up_weight = deepcopy(module.k_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        q_weight = deepcopy(module.q_proj.weight.data).reshape(n_heads, head_dim, hidden_size) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_weight = torch.cat([q_weight,deepcopy(module.q_proj.bias.data).reshape(n_heads, head_dim, 1)],dim=-1)\n",
    "        q_k_up = torch.einsum(\"hdc,hdD->hcD\",k_up_weight, q_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        U,S,V = torch.svd_lowrank(q_k_up, head_dim, niter=16) # U(n_heads, latent_dim, head_dim), S(n_heads, head_dim), V(n_heads, hidden_size, head_dim)\n",
    "        US_sqrt = torch.einsum('hLd,hd->hdL',U,torch.sqrt(S)) # (n_heads, head_dim, latent_dim)\n",
    "        S_sqrtV = torch.einsum('hd,hDd->hdD',torch.sqrt(S),V) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            module.q_proj.bias.data = S_sqrtV[:,:,-1].reshape(-1).contiguous()\n",
    "            S_sqrtV = S_sqrtV[:,:,:-1]\n",
    "        module.k_up_proj.weight.data = US_sqrt.reshape(n_heads*head_dim, latent_dim).contiguous()\n",
    "        module.q_proj.weight.data = S_sqrtV.reshape(n_heads*head_dim, hidden_size).contiguous()\n",
    "\n",
    "        # Orthogonal o_proj and v_up_proj\n",
    "        v_up_weight = deepcopy(module.v_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim)\n",
    "        o_weight = deepcopy(module.o_proj.weight.data).reshape(hidden_size, n_heads, head_dim)\n",
    "        v_up_o = torch.einsum(\"hdc,Dhd->hcD\",v_up_weight, o_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        U,S,V = torch.svd_lowrank(v_up_o, head_dim, niter=16) # U(n_heads, latent_dim, head_dim), S(n_heads, head_dim), V(n_heads, hidden_size, head_dim)\n",
    "        US_sqrt = torch.einsum('hLd,hd->hdL',U,torch.sqrt(S)) # (n_heads, head_dim, latent_dim)\n",
    "        S_sqrtV = torch.einsum('hd,hDd->Dhd',torch.sqrt(S),V) # (hidden_size, n_heads, head_dim)\n",
    "        module.v_up_proj.weight.data = US_sqrt.reshape(hidden_size, latent_dim).contiguous()\n",
    "        module.o_proj.weight.data = S_sqrtV.reshape(hidden_size, n_heads*head_dim).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Tall me a story\n",
      "I am a tall woman. I am 5'10\" and I have been tall my whole life. I have always been the tallest person in the room. I have always been the tallest person in my family. I have always been the tallest person in my class. I have always been the tallest person in my group of friends. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"Tall me a story\",return_tensors=\"pt\").to(\"cuda:1\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,module in model.named_modules():\n",
    "    if name.endswith(\"self_attn\"):\n",
    "        # Absorb k_up_proj into q_proj\n",
    "        k_up_weight = deepcopy(module.k_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        q_weight = deepcopy(module.q_proj.weight.data).reshape(n_heads, head_dim, hidden_size) # (n_heads, head_dim, hidden_size)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_weight = torch.cat([q_weight,deepcopy(module.q_proj.bias.data).reshape(n_heads, head_dim, 1)],dim=-1)\n",
    "        q_k_up = torch.einsum(\"hdc,hdD->hcD\",k_up_weight, q_weight) # (n_heads, latent_dim, hidden_size), rank<=head_dim\n",
    "        q_proj = torch.nn.Linear(hidden_size, n_heads*latent_dim, bias=(module.q_proj.bias is not None))\n",
    "        q_proj = q_proj.to(device=module.q_proj.weight.device, dtype=module.q_proj.weight.dtype)\n",
    "        if module.q_proj.bias is not None:\n",
    "            q_proj.bias.data = q_k_up[:,:,-1].reshape(-1).contiguous()\n",
    "            q_k_up = q_k_up[:,:,:-1]\n",
    "        q_proj.weight.data = q_k_up.reshape(n_heads*latent_dim, hidden_size).contiguous()\n",
    "        setattr(module, \"q_proj\", q_proj)\n",
    "        delattr(module, \"k_up_proj\")\n",
    "        # Absorb v_up_proj into o_proj\n",
    "        v_up_weight = deepcopy(module.v_up_proj.weight.data).reshape(n_heads, head_dim, latent_dim) # (n_heads, head_dim, latent_dim)\n",
    "        o_weight = deepcopy(module.o_proj.weight.data).reshape(hidden_size, n_heads, head_dim) # (n_heads, head_dim, hidden_size)\n",
    "        v_up_o = torch.einsum(\"hdc,Dhd->Dhc\",v_up_weight, o_weight) # (hidden_size, n_heads, latent_dim), rank<=head_dim\n",
    "        o_proj = torch.nn.Linear(n_heads*latent_dim, hidden_size, bias=(module.o_proj.bias is not None))\n",
    "        o_proj = o_proj.to(device=module.o_proj.weight.device, dtype=module.o_proj.weight.dtype)\n",
    "        o_proj.weight.data = v_up_o.reshape(hidden_size, n_heads*latent_dim).contiguous()\n",
    "        if module.o_proj.bias is not None:\n",
    "            o_proj.bias.data = module.o_proj.bias\n",
    "        setattr(module, \"o_proj\", o_proj)\n",
    "        delattr(module, \"v_up_proj\")\n",
    "        module.absorb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Tall me a story\n",
      "I am a tall woman. I am 5'10\" and I have been tall my whole life. I have always been the tallest person in the room. I have always been the tallest person in my family. I have always been the tallest person in my class. I have always been the tallest person in my group of friends. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle. I have always been the tallest person in my social circle\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**tokenizer(\"Tall me a story\",return_tensors=\"pt\").to(\"cuda:1\"), max_new_tokens=500, do_sample=False)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transmla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
